{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Loading and Inspecting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing packages\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load dataset\ntrain = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest  = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature to predict\ntarget = list(set(train.columns) - set(test.columns))\ntarget = target[0] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## X/Y datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.loc[:, train.columns != target]\nY_train = train[target]\n\nX_test = test\n\n#Drop id\nX_test_id = X_test[\"Id\"]\n\nX_train.drop(columns='Id',inplace=True)\nX_test.drop(columns='Id',inplace=True)\n\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fill NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillNa_df(df):\n    \n    #select object columns\n    obj_col = df.columns[df.dtypes == 'object'].values\n\n    #select non object columns\n    num_col = df.columns[df.dtypes != 'object'].values\n\n    #replace null value in obj columns with None\n    df[obj_col] = df[obj_col].fillna('None')\n\n    #replace null value in numeric columns with 0\n    df[num_col] = df[num_col].fillna(0)\n    \n    return df\n\nX_train_001 = fillNa_df(X_train)\nX_test_001 = fillNa_df(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding ordinal/categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ndef oneHotEncoding(df_train, df_test):\n    \n    #select object columns\n    obj_col = df_train.columns[df_train.dtypes == 'object'].values\n\n    # creating instance of one-hot-encoder\n    enc = OneHotEncoder(handle_unknown='ignore')\n\n    # Ordinal features\n    ordinal_features = [x for x in obj_col]\n\n    # passing cat column (label encoded values)\n    df_train_encoded = pd.DataFrame(enc.fit_transform(df_train[ordinal_features]).toarray())\n    df_test_encoded  = pd.DataFrame(enc.transform(df_test[ordinal_features]).toarray())\n    \n    df_train_encoded.reset_index(drop=True, inplace=True)\n    df_test_encoded.reset_index(drop=True, inplace=True)\n\n    # merge with main df\n    df_train_encoded = pd.concat([df_train, df_train_encoded], axis=1)\n    df_test_encoded  = pd.concat([df_test,  df_test_encoded], axis=1)\n\n    # drop ordinal features\n    df_train_encoded.drop(columns=ordinal_features, inplace=True)\n    df_test_encoded.drop(columns=ordinal_features, inplace=True)\n    \n    return df_train_encoded, df_test_encoded\n\nX_train_002, X_test_002 = oneHotEncoding(X_train_001, X_test_001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def featureEng(df):\n\n    #TotalBath\n    df['TotalBath'] = (df['FullBath'] + df['HalfBath'] + df['BsmtFullBath'] + df['BsmtHalfBath'])\n\n    #TotalPorch\n    df['TotalPorch'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\n\n    #Modeling happen during the sale year\n    df[\"RecentRemodel\"] = (df[\"YearRemodAdd\"] == df[\"YrSold\"]) * 1\n\n    #House sold in the year it was built\n    df[\"NewHouse\"] = (df[\"YearBuilt\"] == df[\"YrSold\"]) * 1\n\n    #YrBltAndRemod\n    df[\"YrBltAndRemod\"] = df[\"YearBuilt\"] + df[\"YearRemodAdd\"]\n\n    #Total_sqr_footage\n    df[\"Total_sqr_footage\"] = df[\"BsmtFinSF1\"] + df[\"BsmtFinSF2\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\n    df['Area_Qual'] = df['TotalSF'] * df['OverallQual']\n\n    #HasPool\n    df['HasPool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasFireplaces\n    df['HasFirePlace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n    #Has2ndFloor\n    df['Has2ndFloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasGarage\n    df['HasGarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasBsmnt\n    df['HasBsmnt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    \n    return df\n\n#Feature Engineering\nX_train_003 = featureEng(X_train_002)\nX_test_003 = featureEng(X_test_002)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outlier Detection\n\nPerhaps the most important hyperparameter in the model is the “contamination” argument, which is used to help estimate the number of outliers in the dataset. This is a value between 0.0 and 0.5 and by default is set to 0.1."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\n\ndef dropoutlier(df_X, df_Y, method='IsolationForest'):\n\n    if(method=='IsolationForest'):\n        #Isolation Forest\n\n        # identify outliers in the training dataset\n        iso = IsolationForest(contamination=0.012)\n        yhat = iso.fit_predict(df_X)\n\n    if(method=='MinimumCovarianceDeterminant'):\n        #Minimum Covariance Determinant\n\n        # identify outliers in the training dataset\n        ee = EllipticEnvelope(contamination=0.01, n_jobs=-1, random_state=42)\n        yhat = ee.fit_predict(df_X)\n\n    if(method=='LocalOutlierFactor'):\n        #Local Outlier Factor\n\n        # identify outliers in the training dataset\n        lof = LocalOutlierFactor()\n        yhat = lof.fit_predict(df_X)\n\n    if(method=='OneClassSVM'):\n        #One-Class SVM\n\n        # identify outliers in the training dataset\n        ee = OneClassSVM(nu=0.001)\n        yhat = ee.fit_predict(df_X)\n\n    # select all rows that are not outliers\n    mask = yhat != -1\n    df_X_drop, df_Y_drop = df_X[mask], df_Y[mask]\n\n    # select all rows that are outliers\n    masko = yhat == -1\n    df_X_o, df_Y_o = df_X[masko], df_Y[masko]\n    \n    return df_X, df_Y, [df_X_o, df_Y_o]\n\n#Drop outliers\nX_train_004, Y_train_004, df_o = dropoutlier(X_train_003, Y_train)\n\n# summarize the shape of the updated training dataset\nprint('Total: ', X_train_004.shape)\nprint('Not Outliers: ', X_train_003.shape)\nprint('Outliers: ', df_o[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot GrLivArea vs SalePrice\nplt.scatter(X_train_004['GrLivArea'], Y_train_004, color='blue', alpha=0.5)\nplt.scatter(df_o[0]['GrLivArea'],   df_o[1],   color='red',  alpha=0.5, label='Outlier')\nplt.legend(loc=\"upper left\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split dataframe - Train Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing packages\nfrom sklearn.model_selection import train_test_split\n\n#Particiona o data set originalmente Train em Train(Treino) e Val(validação)\nX_train_005, X_val_005, Y_train_005, Y_val_005 = train_test_split(X_train_004, \n                                                                  Y_train_004, \n                                                                  test_size=0.2, \n                                                                  random_state=42)\n\nX_train_005.shape, X_val_005.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 5. Fit Models"},{"metadata":{},"cell_type":"markdown","source":"## 5.2 XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Packages\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom skopt import BayesSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.preprocessing import Imputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"https://www.kaggle.com/nanomathias/bayesian-optimization-of-xgboost-lb-0-9769","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_cv_tuner = BayesSearchCV(\n    estimator = XGBRegressor(\n        n_jobs = 1,\n        objective = 'reg:squarederror',\n        eval_metric = 'mae',\n        silent=1,\n        tree_method='gpu_hist'\n    ),\n    search_spaces = {\n        'learning_rate': (0.01, 1.0, 'log-uniform'),\n        'min_child_weight': (0, 10),\n        'max_depth': (0, 50),\n        'max_delta_step': (0, 20),\n        'subsample': (0.01, 1.0, 'uniform'),\n        'colsample_bytree': (0.01, 1.0, 'uniform'),\n        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'gamma': (1e-9, 0.5, 'log-uniform'),\n        'min_child_weight': (0, 5),\n        'n_estimators': (50, 100),\n        'scale_pos_weight': (1e-6, 500, 'log-uniform')\n    },    \n    scoring = 'mae',\n    cv = StratifiedKFold(\n        n_splits=3,\n        shuffle=True,\n        random_state=42\n    ),\n    n_jobs = 3,\n    n_iter = 10,   \n    verbose = 0,\n    refit = True,\n    random_state = 42\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [200, 500],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = XGBRegressor(tree_method='gpu_hist')\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run only in the first run of the kernel.\n#hyperParameterTuning(X_train_005, Y_train_005)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Params\n<br>{'colsample_bytree': 0.7,\n 'learning_rate': 0.1,\n 'max_depth': 3,\n 'min_child_weight': 1,\n 'n_estimators': 500,\n 'objective': 'reg:squarederror',\n 'subsample': 0.7}"},{"metadata":{},"cell_type":"markdown","source":"### Best Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model fit\nxgb_model = XGBRegressor(\n        objective = 'reg:squarederror',\n        colsample_bytree = 0.7,\n        learning_rate = 0.1,\n        max_depth = 3,\n        min_child_weight = 1,\n        n_estimators = 500,\n        subsample = 0.7)\n\neval_set = [(X_val_005, Y_val_005)]\n\nxgb_model.fit(X_train_005, Y_train_005, early_stopping_rounds=10, eval_metric=\"mae\", eval_set = eval_set, verbose=False)\n\nprint('MAE: ', xgb_model.evals_result()['validation_0']['mae'][-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_val_pred = xgb_model.predict(X_val_005)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Plot Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot Real vs Predict\nplt.scatter(X_val_005['GrLivArea'], Y_val_005,   color='blue', label='Real',    alpha=0.5)\nplt.scatter(X_val_005['GrLivArea'], Y_val_pred,  color='red' , label='Predict', alpha=0.5)\nplt.title(\"Real vs Predict\")\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Predic Test & Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the model to make predictions\nY_pred_test = xgb_model.predict(X_test_003)\n\nsubmission = pd.DataFrame({'Id':X_test_id,'SalePrice':Y_pred_test})\n\n# Save results\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}